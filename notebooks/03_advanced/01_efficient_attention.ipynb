{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Efficient Attention: Flash Attention & Memory Optimization\n",
    "\n",
    "Attention layers become the bottleneck when sequence lengths grow. This notebook examines memory-efficient variants—most notably Flash Attention—and shows how to benchmark, mask, and fall back to chunked implementations when kernels are unavailable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Diagnose the computational cost of naive attention.\n",
    "- Use PyTorch 2.x `scaled_dot_product_attention` to leverage Flash/efficient kernels when available.\n",
    "- Implement chunked attention as a fallback for long sequences.\n",
    "- Build an attention wrapper that records backend choice and timing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost of Naive Attention\n",
    "\n",
    "Standard attention materializes the full `L × L` score matrix, consuming `O(L^2)` memory and compute. Flash Attention algorithms compute softmax in tiles, reducing memory to `O(L)` while keeping results numerically stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "def naive_attention(q, k, v, mask=None):\n",
    "    scores = q @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return weights @ v\n",
    "\n",
    "q = torch.randn(2, 8, 128, 64)\n",
    "k = torch.randn(2, 8, 128, 64)\n",
    "v = torch.randn(2, 8, 128, 64)\n",
    "\n",
    "out_naive = naive_attention(q, k, v)\n",
    "out_flash = F.scaled_dot_product_attention(q, k, v)\n",
    "print((out_naive - out_flash).abs().max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking Runtime\n",
    "\n",
    "On CPU the difference is modest; on GPUs Flash Attention shines. Still, benchmarking locally helps you understand the trade-offs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(fn, *args, iters=20):\n",
    "    start = time.perf_counter()\n",
    "    for _ in range(iters):\n",
    "        fn(*args)\n",
    "    end = time.perf_counter()\n",
    "    return (end - start) / iters\n",
    "\n",
    "naive_time = benchmark(naive_attention, q, k, v)\n",
    "flash_time = benchmark(F.scaled_dot_product_attention, q, k, v)\n",
    "print(f\"Naive: {naive_time:.6f}s | Flash/efficient: {flash_time:.6f}s (CPU measurement; expect larger gains on GPU)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Sparsity Masks\n",
    "\n",
    "Local attention windows restrict computation to a band around the diagonal—a common trick in long sequence models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 64\n",
    "radius = 8\n",
    "mask = torch.zeros(seq_len, seq_len)\n",
    "for i in range(seq_len):\n",
    "    mask[i, max(0, i - radius): i + radius + 1] = 1\n",
    "\n",
    "plt.imshow(mask, cmap=\"Blues\")\n",
    "plt.title(\"Local attention mask (radius=8)\")\n",
    "plt.xlabel(\"Key index\")\n",
    "plt.ylabel(\"Query index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Chunked Attention\n",
    "\n",
    "Implement attention in chunks along the query dimension to reduce memory usage when Flash kernels are unavailable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_attention(q, k, v, chunk_size=32):\n",
    "    # TODO: compute attention by iterating over query chunks\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def chunked_attention(q, k, v, chunk_size=32):\n",
    "    outputs = []\n",
    "    for start in range(0, q.size(-2), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        q_chunk = q[..., start:end, :]\n",
    "        attn = q_chunk @ k.transpose(-2, -1) / math.sqrt(q.size(-1))\n",
    "        weights = torch.softmax(attn, dim=-1)\n",
    "        outputs.append(weights @ v)\n",
    "    return torch.cat(outputs, dim=-2)\n",
    "\n",
    "chunked = chunked_attention(q, k, v)\n",
    "print((chunked - out_naive).abs().max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Flash Attention Wrapper\n",
    "\n",
    "Create a module `FlashMHA` that:\n",
    "\n",
    "- Projects inputs into Q/K/V, runs attention.\n",
    "- Uses PyTorch's scaled dot-product kernels when available and falls back to chunked or naive attention otherwise.\n",
    "- Records which backend was used and the elapsed time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlashMHA(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # TODO: project inputs, choose attention backend, record stats\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class FlashMHA(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.0):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def _reshape(self, tensor):\n",
    "        bsz, seq_len, _ = tensor.shape\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        bsz, seq_len, _ = x.shape\n",
    "        q = self._reshape(self.q_proj(x))\n",
    "        k = self._reshape(self.k_proj(x))\n",
    "        v = self._reshape(self.v_proj(x))\n",
    "        attn_mask = mask\n",
    "        if mask is not None and mask.dim() == 3:\n",
    "            attn_mask = mask.unsqueeze(1)\n",
    "\n",
    "        start = time.perf_counter()\n",
    "        backend = \"flash\"\n",
    "        try:\n",
    "            out = F.scaled_dot_product_attention(q, k, v, attn_mask=attn_mask)\n",
    "        except RuntimeError:\n",
    "            backend = \"chunked\"\n",
    "            out = chunked_attention(q, k, v)\n",
    "        elapsed_ms = (time.perf_counter() - start) * 1000\n",
    "\n",
    "        out = out.transpose(1, 2).contiguous().view(bsz, seq_len, self.embed_dim)\n",
    "        out = self.out_proj(self.dropout(out))\n",
    "        stats = {\"backend\": backend, \"elapsed_ms\": elapsed_ms}\n",
    "        return out, stats\n",
    "\n",
    "flash_mha = FlashMHA(embed_dim=64, num_heads=4)\n",
    "dummy = torch.randn(2, 32, 64)\n",
    "out, stats = flash_mha(dummy)\n",
    "print(out.shape, stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- Dao et al. (2022) “FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness”\n",
    "- PyTorch 2.1 release notes for SDPA kernels\n",
    "- Triton tutorials for custom GPU kernels\n",
    "- Long-range attention models such as Longformer and Performer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
