{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixture of Experts (MoE) Architectures\n",
    "\n",
    "Mixture of Experts (MoE) models route tokens to specialized subnetworks, scaling parameter counts without linearly increasing compute. This notebook builds sparse gating, dispatch logic, and load-balancing losses in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Implement top-k routing that selects a subset of experts per token.\n",
    "- Dispatch tokens to experts, accumulate outputs, and combine them with router probabilities.\n",
    "- Apply load-balancing and entropy penalties to prevent expert collapse.\n",
    "- Construct a reusable MoE feed-forward layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "class TopKRouter(nn.Module):\n",
    "    def __init__(self, model_dim, num_experts, k=2):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        self.linear = nn.Linear(model_dim, num_experts)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear(x)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        topk_probs, topk_idx = torch.topk(probs, self.k, dim=-1)\n",
    "        return topk_probs, topk_idx, probs\n",
    "\n",
    "router = TopKRouter(32, 4, k=2)\n",
    "tokens = torch.randn(8, 32)\n",
    "scores, indices, probs = router(tokens)\n",
    "print(scores.shape, indices.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experts and Dispatch\n",
    "\n",
    "Experts are typically lightweight feed-forward modules. We dispatch each token to its selected experts, apply expert transformations, and combine outputs with router weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Expert(nn.Module):\n",
    "    def __init__(self, model_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(model_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(hidden_dim, model_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "experts = nn.ModuleList([Expert(32, 64) for _ in range(4)])\n",
    "\n",
    "def moe_forward(x, router, experts):\n",
    "    topk_probs, topk_idx, _ = router(x)\n",
    "    outputs = torch.zeros_like(x)\n",
    "    for i, (weights, expert_ids, token) in enumerate(zip(topk_probs, topk_idx, x)):\n",
    "        combined = 0.0\n",
    "        for w, expert_id in zip(weights, expert_ids):\n",
    "            combined += w * experts[expert_id](token.unsqueeze(0)).squeeze(0)\n",
    "        outputs[i] = combined\n",
    "    return outputs\n",
    "\n",
    "outputs = moe_forward(tokens, router, experts)\n",
    "print(outputs.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Balancing Loss\n",
    "\n",
    "Routers can collapse to a small subset of experts. Encourage uniform utilization with a KL divergence penalty and visualize average probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_balancing_loss(probs, epsilon=1e-9):\n",
    "    mean_usage = probs.mean(dim=0)\n",
    "    uniform = torch.full_like(mean_usage, 1.0 / mean_usage.numel())\n",
    "    kl = (uniform * (uniform.add(epsilon).log() - mean_usage.add(epsilon).log())).sum()\n",
    "    return kl\n",
    "\n",
    "lb_loss = load_balancing_loss(probs)\n",
    "print(f\"Load balancing loss: {lb_loss.item():.6f}\")\n",
    "\n",
    "avg_probs = probs.mean(dim=0).detach()\n",
    "plt.bar(range(len(avg_probs)), avg_probs)\n",
    "plt.xlabel(\"Expert ID\")\n",
    "plt.ylabel(\"Average probability\")\n",
    "plt.title(\"Expert utilization snapshot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Capacity Limits\n",
    "\n",
    "Real MoE implementations cap the number of tokens each expert processes (capacity). Implement a dispatcher that enforces a maximum capacity per expert and reports any dropped tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moe_forward_with_capacity(x, router, experts, capacity=2):\n",
    "    # TODO: dispatch tokens with capacity limits; print dropped tokens\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def moe_forward_with_capacity(x, router, experts, capacity=2):\n",
    "    topk_probs, topk_idx, _ = router(x)\n",
    "    expert_buffers = {idx: [] for idx in range(len(experts))}\n",
    "    outputs = torch.zeros_like(x)\n",
    "    for token_idx, (weights, ids, token) in enumerate(zip(topk_probs, topk_idx, x)):\n",
    "        dispatched = False\n",
    "        for weight, expert_id in zip(weights, ids):\n",
    "            if len(expert_buffers[expert_id]) < capacity:\n",
    "                outputs[token_idx] += weight * experts[expert_id](token.unsqueeze(0)).squeeze(0)\n",
    "                expert_buffers[expert_id].append(token_idx)\n",
    "                dispatched = True\n",
    "                break\n",
    "        if not dispatched:\n",
    "            print(f\"Token {token_idx} dropped due to capacity limits\")\n",
    "    return outputs\n",
    "\n",
    "moe_forward_with_capacity(tokens, router, experts, capacity=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Enhancements\n",
    "\n",
    "- **Switch Transformers** route each token to a single expert (k=1) for simplicity.\n",
    "- **GShard** and **GLaM** scale MoE layers across multiple devices with specialized load-balancing losses.\n",
    "- **Weighted load balancing** adds entropy regularization to encourage exploration of experts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – MoE Feed-Forward Layer\n",
    "\n",
    "Build an `MoEFeedForward` module compatible with transformer feed-forward blocks. The module should:\n",
    "\n",
    "- Accept a temperature parameter to anneal router logits.\n",
    "- Return the combined output along with auxiliary metrics (load balancing loss, entropy, expert usage).\n",
    "- Support configuring number of experts, top-k routing, and hidden dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoEFeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, hidden_dim, num_experts, k=2, router_temp=1.0):\n",
    "        super().__init__()\n",
    "        # TODO: initialize router, experts, and temperature parameter\n",
    "\n",
    "    def forward(self, x, return_aux=False):\n",
    "        # TODO: apply routing, combine expert outputs, compute aux metrics\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class MoEFeedForward(nn.Module):\n",
    "    def __init__(self, model_dim, hidden_dim, num_experts, k=2, router_temp=1.0):\n",
    "        super().__init__()\n",
    "        self.router = TopKRouter(model_dim, num_experts, k)\n",
    "        self.experts = nn.ModuleList([Expert(model_dim, hidden_dim) for _ in range(num_experts)])\n",
    "        self.router_temp = nn.Parameter(torch.tensor(router_temp, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x, return_aux=False):\n",
    "        scaled_input = x / self.router_temp.clamp(min=0.1)\n",
    "        topk_probs, topk_idx, probs = self.router(scaled_input)\n",
    "        outputs = torch.zeros_like(x)\n",
    "        expert_counts = torch.zeros(len(self.experts), device=x.device)\n",
    "        for i, (weights, ids, token) in enumerate(zip(topk_probs, topk_idx, x)):\n",
    "            for weight, expert_id in zip(weights, ids):\n",
    "                outputs[i] += weight * self.experts[expert_id](token.unsqueeze(0)).squeeze(0)\n",
    "                expert_counts[expert_id] += 1\n",
    "        aux = {}\n",
    "        if return_aux:\n",
    "            lb = load_balancing_loss(probs)\n",
    "            entropy = -(probs * (probs + 1e-9).log()).sum(dim=-1).mean()\n",
    "            aux = {\n",
    "                \"load_balance\": lb,\n",
    "                \"entropy\": entropy,\n",
    "                \"expert_usage\": expert_counts / expert_counts.sum().clamp(min=1.0),\n",
    "            }\n",
    "        return outputs, aux\n",
    "\n",
    "moe_layer = MoEFeedForward(32, 64, num_experts=4)\n",
    "out, aux = moe_layer(tokens, return_aux=True)\n",
    "print(out.shape, aux)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- Lepikhin et al. (2020) “GShard: Scaling Giant Models with Conditional Computation”\n",
    "- Fedus et al. (2021) “Switch Transformers: Scaling to Trillion Parameter Models”\n",
    "- Shazeer et al. (2017) “Outrageously Large Neural Networks”\n",
    "- DeepSpeed MoE and Fairseq MoE tutorials for distributed training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
