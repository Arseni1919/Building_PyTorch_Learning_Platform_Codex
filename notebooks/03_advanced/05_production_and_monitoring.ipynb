{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Deployment & Monitoring\n",
    "\n",
    "Shipping a model requires more than training: you must serialize artifacts, optimize them for deployment, and monitor live performance. This notebook walks through TorchScript/ONNX export, quantization, and monitoring hooks such as latency tracking and drift detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Export PyTorch models to TorchScript and ONNX formats.\n",
    "- Apply dynamic quantization for lightweight CPU inference.\n",
    "- Benchmark latency and log alerts for production metrics.\n",
    "- Draft config snippets for inference services (e.g., TorchServe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TorchScript Export\n",
    "\n",
    "TorchScript captures models for deployment in C++ or TorchServe. The script below traces and scripts a simple model, then saves and reloads it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InferenceModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(16, 32), nn.ReLU(), nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = InferenceModel().eval()\n",
    "sample = torch.randn(1, 16)\n",
    "\n",
    "traced = torch.jit.trace(model, sample)\n",
    "scripted = torch.jit.script(model)\n",
    "torch.jit.save(traced, \"notebooks/03_advanced/inference_model_traced.pt\")\n",
    "reloaded = torch.jit.load(\"notebooks/03_advanced/inference_model_traced.pt\")\n",
    "print(\"Traced output\", reloaded(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ONNX Export\n",
    "\n",
    "ONNX provides cross-framework portability, enabling optimization tools like TensorRT or ONNX Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.onnx.export(\n",
    "    model,\n",
    "    sample,\n",
    "    \"notebooks/03_advanced/inference_model.onnx\",\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n",
    "    opset_version=17,\n",
    ")\n",
    "print(\"ONNX export complete\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "Dynamic quantization reduces model size and improves CPU latency. It works well for linear-heavy models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized = torch.quantization.quantize_dynamic(model, {nn.Linear}, dtype=torch.qint8)\n",
    "print(\"Quantized output\", quantized(sample))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latency Benchmarking\n",
    "\n",
    "Measure median, p99, and average latency to set realistic service-level objectives (SLOs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "def benchmark_inference(model, runs=50):\n",
    "    latencies = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(runs):\n",
    "            start = time.perf_counter()\n",
    "            model(sample)\n",
    "            latencies.append((time.perf_counter() - start) * 1000)\n",
    "    return {\n",
    "        \"p50_ms\": statistics.median(latencies),\n",
    "        \"p99_ms\": sorted(latencies)[int(0.99 * len(latencies)) - 1],\n",
    "        \"avg_ms\": sum(latencies) / len(latencies),\n",
    "    }\n",
    "\n",
    "stats = benchmark_inference(traced)\n",
    "print(stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Drift Detection Snapshot\n",
    "\n",
    "Simulate feature drift by shifting live data means relative to training baselines. Plot mean shifts per feature to flag which dimensions changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mean = torch.zeros(16)\n",
    "prod_samples = torch.randn(256, 16) + 0.3  # simulate drift\n",
    "# TODO: compute mean shifts and visualize per feature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "mean_diff = prod_samples.mean(dim=0) - train_mean\n",
    "plt.bar(range(len(mean_diff)), mean_diff.numpy())\n",
    "plt.xlabel(\"Feature index\")\n",
    "plt.ylabel(\"Mean shift\")\n",
    "plt.title(\"Feature drift snapshot\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alerting Hooks\n",
    "\n",
    "Implement logic that inspects latency metrics and flags anomalies (e.g., high p99)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_latency_thresholds(metrics, latency_ms=50):\n",
    "    alerts = []\n",
    "    if metrics[\"p99_ms\"] > latency_ms:\n",
    "        alerts.append(\"High latency\")\n",
    "    if abs(metrics[\"avg_ms\"] - metrics[\"p50_ms\"]) > latency_ms:\n",
    "        alerts.append(\"Latency variance\")\n",
    "    return alerts\n",
    "\n",
    "print(check_latency_thresholds(stats))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – TorchServe Config Snippet\n",
    "\n",
    "Draft a configuration string for TorchServe's `config.properties`, enabling basic metrics and batch inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_properties = \"\"\"\n",
    "# TODO: populate TorchServe config properties\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "config_properties = \"\"\"\n",
    "inference_address=http://0.0.0.0:8080\n",
    "management_address=http://0.0.0.0:8081\n",
    "metrics_address=http://0.0.0.0:8082\n",
    "default_workers_per_model=2\n",
    "batch_size=8\n",
    "max_batch_delay=100\n",
    "enable_metrics_api=true\n",
    "\"\"\"\n",
    "print(config_properties)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Production Playbook\n",
    "\n",
    "Create a Markdown string documenting a deployment checklist: artifact versioning, canary rollout, health checks, drift monitoring, and rollback strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def production_playbook():\n",
    "    # TODO: return multi-line markdown checklist\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def production_playbook():\n",
    "    return \"\"\"\\\n",
    "## Deployment Playbook\n",
    "\n",
    "- [ ] Capture model metadata (commit hash, training data snapshot, evaluation metrics).\n",
    "- [ ] Validate TorchScript/ONNX artifacts with unit and integration tests.\n",
    "- [ ] Deploy a canary instance and compare live metrics against baseline.\n",
    "- [ ] Monitor latency (p50/p95/p99), error rates, and drift scores in real time.\n",
    "- [ ] Configure automated rollback if metrics breach SLA thresholds.\n",
    "- [ ] Schedule a post-deployment review and document learnings.\n",
    "\"\"\"\n",
    "\n",
    "print(production_playbook())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- TorchServe documentation: https://pytorch.org/serve/\n",
    "- ONNX Runtime tuning guides for CPU/GPU inference\n",
    "- ML observability platforms (Arize, WhyLabs, Fiddler) for advanced monitoring\n",
    "- “Hidden Technical Debt in Machine Learning Systems” (Sculley et al.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
