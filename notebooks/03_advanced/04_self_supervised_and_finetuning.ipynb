{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Supervised Learning & Fine-Tuning Strategies\n",
    "\n",
    "Self-supervised pretraining unlocks representations that transfer across tasks. This notebook explores contrastive and masked modeling objectives, then demonstrates adapter-style fine-tuning with differential learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Implement contrastive learning components (augmentations, projection head, InfoNCE loss).\n",
    "- Outline masked prediction objectives for language modeling.\n",
    "- Apply differential learning rates and adapters during fine-tuning.\n",
    "- Build a fine-tuning helper that supports freezing, adapters, and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Learning Primer\n",
    "\n",
    "SimCLR-style contrastive learning pulls together positive views of the same sample while pushing apart negatives. Components:\n",
    "\n",
    "1. Data augmentation pipeline to create positive pairs.\n",
    "2. Encoder that maps inputs to latent vectors.\n",
    "3. Projection head that improves the loss landscape.\n",
    "4. InfoNCE loss over cosine similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class ProjectionHead(nn.Module):\n",
    "    def __init__(self, input_dim, proj_dim=64):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, proj_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(proj_dim, proj_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.normalize(self.net(x), dim=-1)\n",
    "\n",
    "encoder = nn.Sequential(nn.Linear(128, 256), nn.ReLU(), nn.Linear(256, 128))\n",
    "projector = ProjectionHead(128, 64)\n",
    "\n",
    "def info_nce_loss(z_i, z_j, temperature=0.1):\n",
    "    batch = z_i.size(0)\n",
    "    representations = torch.cat([z_i, z_j], dim=0)\n",
    "    similarity = F.cosine_similarity(representations.unsqueeze(1), representations.unsqueeze(0), dim=-1)\n",
    "    labels = torch.arange(batch, device=z_i.device)\n",
    "    labels = torch.cat([labels, labels], dim=0)\n",
    "    mask = torch.eye(2 * batch, device=z_i.device, dtype=torch.bool)\n",
    "    similarity = similarity.masked_fill(mask, float('-inf'))\n",
    "    logits = similarity / temperature\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    return loss\n",
    "\n",
    "views_1 = torch.randn(32, 128)\n",
    "views_2 = torch.randn(32, 128)\n",
    "z1 = projector(encoder(views_1))\n",
    "z2 = projector(encoder(views_2))\n",
    "print(f\"InfoNCE loss: {info_nce_loss(z1, z2):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Cross-View Similarities\n",
    "\n",
    "Positive pairs should trend toward high similarity (bright diagonal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = torch.mm(z1, z2.t()).detach()\n",
    "plt.imshow(sim_matrix, cmap=\"viridis\")\n",
    "plt.colorbar(label=\"cosine similarity\")\n",
    "plt.title(\"Cross-view similarity matrix\")\n",
    "plt.xlabel(\"View 2 index\")\n",
    "plt.ylabel(\"View 1 index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked Prediction Quickstart\n",
    "\n",
    "Masked language modeling predicts missing tokens. Use a sentinel (e.g., vocabulary index for `[MASK]`) and ensure the loss ignores non-masked positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 100\n",
    "embed = nn.Embedding(vocab_size, 32)\n",
    "classifier = nn.Linear(32, vocab_size)\n",
    "tokens = torch.randint(0, vocab_size, (4, 10))\n",
    "mask = torch.rand_like(tokens.float()) < 0.3\n",
    "masked_tokens = tokens.clone()\n",
    "masked_tokens[mask] = vocab_size - 1  # assume last index is [MASK]\n",
    "embeddings = embed(masked_tokens)\n",
    "logits = classifier(embeddings)\n",
    "loss = F.cross_entropy(logits[mask], tokens[mask])\n",
    "print(f\"Masked LM loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Adapter Layer\n",
    "\n",
    "Adapters insert a bottleneck module (down-project → nonlinearity → up-project) into existing layers. Implement an adapter and a helper that injects adapters into every `nn.Linear` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim, bottleneck=32):\n",
    "        super().__init__()\n",
    "        # TODO: define down, activation, up projections\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "def add_adapters(module, bottleneck=32):\n",
    "    # TODO: recursively wrap linear layers with adapters\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Adapter(nn.Module):\n",
    "    def __init__(self, dim, bottleneck=32):\n",
    "        super().__init__()\n",
    "        self.down = nn.Linear(dim, bottleneck)\n",
    "        self.activation = nn.GELU()\n",
    "        self.up = nn.Linear(bottleneck, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.up(self.activation(self.down(x)))\n",
    "\n",
    "def add_adapters(module, bottleneck=32):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            wrapped = nn.Sequential(child, Adapter(child.out_features, bottleneck))\n",
    "            setattr(module, name, wrapped)\n",
    "        else:\n",
    "            add_adapters(child, bottleneck)\n",
    "    return module\n",
    "\n",
    "encoder_with_adapters = add_adapters(encoder, bottleneck=16)\n",
    "print(encoder_with_adapters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential Learning Rates\n",
    "\n",
    "When fine-tuning, use smaller learning rates for pretrained layers and larger ones for new heads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(model, head, base_lr=1e-4, head_lr=1e-3):\n",
    "    return torch.optim.Adam([\n",
    "        {\"params\": [p for p in model.parameters() if p.requires_grad], \"lr\": base_lr},\n",
    "        {\"params\": head.parameters(), \"lr\": head_lr},\n",
    "    ])\n",
    "\n",
    "head = nn.Linear(128, 5)\n",
    "optimizer = create_optimizer(encoder_with_adapters, head)\n",
    "print([group[\"lr\"] for group in optimizer.param_groups])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Fine-Tuning Helper\n",
    "\n",
    "Implement a `FineTuner` class that can freeze layers, insert adapters, apply differential learning rates, and evaluate accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FineTuner:\n",
    "    def __init__(self, encoder, head, freeze_until=None, adapter_bottleneck=None, base_lr=1e-4, head_lr=1e-3):\n",
    "        # TODO: clone encoder, freeze layers, add adapters, create optimizer\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class FineTuner:\n",
    "    def __init__(self, encoder, head, freeze_until=None, adapter_bottleneck=None, base_lr=1e-4, head_lr=1e-3):\n",
    "        self.encoder = encoder\n",
    "        if freeze_until is not None:\n",
    "            for name, param in self.encoder.named_parameters():\n",
    "                param.requires_grad = freeze_until not in name\n",
    "        if adapter_bottleneck is not None:\n",
    "            add_adapters(self.encoder, adapter_bottleneck)\n",
    "        self.head = head\n",
    "        params = [\n",
    "            {\"params\": [p for p in self.encoder.parameters() if p.requires_grad], \"lr\": base_lr},\n",
    "            {\"params\": self.head.parameters(), \"lr\": head_lr},\n",
    "        ]\n",
    "        self.optimizer = torch.optim.AdamW(params)\n",
    "\n",
    "    def train_step(self, batch):\n",
    "        self.encoder.train()\n",
    "        self.head.train()\n",
    "        xb, yb = batch\n",
    "        feats = self.encoder(xb)\n",
    "        logits = self.head(feats)\n",
    "        loss = F.cross_entropy(logits, yb)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.encoder.eval()\n",
    "        self.head.eval()\n",
    "        correct = total = 0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in loader:\n",
    "                preds = self.head(self.encoder(xb)).argmax(dim=-1)\n",
    "                correct += (preds == yb).sum().item()\n",
    "                total += yb.numel()\n",
    "        return correct / max(total, 1)\n",
    "\n",
    "tuner = FineTuner(encoder_with_adapters, head, adapter_bottleneck=16)\n",
    "dummy_loader = [(torch.randn(16, 128), torch.randint(0, 5, (16,))) for _ in range(2)]\n",
    "for batch in dummy_loader:\n",
    "    loss = tuner.train_step(batch)\n",
    "acc = tuner.evaluate(dummy_loader)\n",
    "print(f\"Accuracy: {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- Chen et al. (2020) “A Simple Framework for Contrastive Learning of Visual Representations” (SimCLR)\n",
    "- He et al. (2021) “Masked Autoencoders Are Scalable Vision Learners” (MAE)\n",
    "- Hu et al. (2022) “LoRA: Low-Rank Adaptation of Large Language Models”\n",
    "- OpenAI Fine-Tuning cookbooks and Hugging Face adapter libraries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
