{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85f66ccd",
   "metadata": {},
   "source": [
    "# Data Pipelines with PyTorch\n",
    "\n",
    "Reliable data pipelines keep your models productive. In this notebook we design loaders that scale from quick experiments to production workloads while respecting the top-down workflow introduced earlier.\n",
    "\n",
    "_Environment note:_ Guidance reflects PyTorch practices current through October 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9729199",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Decide when to build a custom `Dataset` versus using built-ins.\n",
    "- Compose deterministic transforms and stochastic augmentations.\n",
    "- Configure `DataLoader` workers, shuffling, and memory pinning.\n",
    "- Prepare tabular, sequential, and vision-friendly batches for the notebooks ahead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5270ede2",
   "metadata": {},
   "source": [
    "## Pipeline Stages at a Glance\n",
    "\n",
    "1. **Ingest** raw samples.\n",
    "2. **Transform** them into tensors (normalize, tokenize, augment).\n",
    "3. **Batch/collate** to align shapes.\n",
    "4. **Prefetch & transfer** so computation and I/O overlap.\n",
    "\n",
    "Keep this map handy when diagnosing bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97971640",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 2.5))\n",
    "ax.axis(\"off\")\n",
    "\n",
    "stages = [\n",
    "    (0.05, 0.6, \"Ingest\"),\n",
    "    (0.30, 0.6, \"Transform\"),\n",
    "    (0.55, 0.6, \"Batch\"),\n",
    "    (0.80, 0.6, \"Prefetch\"),\n",
    "]\n",
    "\n",
    "for x, y, label in stages:\n",
    "    ax.add_patch(plt.Rectangle((x, y), 0.18, 0.25, color=\"#d3e1ff\", ec=\"#3366cc\", lw=2))\n",
    "    ax.text(x + 0.09, y + 0.125, label, ha=\"center\", va=\"center\", fontsize=11)\n",
    "\n",
    "for (x, _, _), (nx, _, _) in zip(stages[:-1], stages[1:]):\n",
    "    ax.annotate(\"\", xy=(nx, 0.72), xytext=(x + 0.18, 0.72), arrowprops=dict(arrowstyle=\"->\", lw=2))\n",
    "\n",
    "ax.text(0.5, 0.22, \"A saturated pipeline keeps accelerators busy\", ha=\"center\", fontsize=11)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7a4dfd",
   "metadata": {},
   "source": [
    "## Custom Dataset Example (Tabular Data)\n",
    "\n",
    "Convert NumPy arrays to tensors once in `__init__` to avoid unnecessary work in every `__getitem__` call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7637e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, features: np.ndarray, targets: np.ndarray):\n",
    "        self.x = torch.from_numpy(features).float()\n",
    "        self.y = torch.from_numpy(targets).float().unsqueeze(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "features = rng.normal(size=(128, 3))\n",
    "targets = (features @ np.array([0.5, -1.2, 2.0]) + 0.3).astype(np.float32)\n",
    "\n",
    "dataset = TabularDataset(features, targets)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "\n",
    "xb, yb = next(iter(loader))\n",
    "print(xb.shape, yb.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cb011",
   "metadata": {},
   "source": [
    "### Visual Sanity Checks\n",
    "\n",
    "Spot outliers or skewed distributions before training begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a8933",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(9, 3))\n",
    "for idx, ax in enumerate(axes):\n",
    "    ax.hist(features[:, idx], bins=15, color=\"#4c72b0\", alpha=0.75)\n",
    "    ax.set_title(f\"Feature {idx}\")\n",
    "fig.suptitle(\"Feature Distributions\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247cb54",
   "metadata": {},
   "source": [
    "## Mini Task – Padding Variable-Length Sequences\n",
    "\n",
    "Sequence models (next notebook) need padded batches. Implement a collate function that returns both padded sequences and their original lengths.\n",
    "\n",
    "Try the starter cell before revealing the hidden solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9919406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "\n",
    "toy_sequences = [[1, 2, 3], [4, 5], [6]]\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __len__(self):\n",
    "        return len(toy_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(toy_sequences[idx], dtype=torch.long)\n",
    "\n",
    "# TODO: implement collate_fn that returns (padded_batch, lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ba0880",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "toy_sequences = [[1, 2, 3], [4, 5], [6]]\n",
    "\n",
    "class ToyDataset(Dataset):\n",
    "    def __len__(self):\n",
    "        return len(toy_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(toy_sequences[idx], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    lengths = torch.tensor([item.size(0) for item in batch])\n",
    "    padded = pad_sequence(batch, batch_first=True, padding_value=0)\n",
    "    return padded, lengths\n",
    "\n",
    "loader = DataLoader(ToyDataset(), batch_size=3, collate_fn=collate_fn)\n",
    "padded, lengths = next(iter(loader))\n",
    "print(padded)\n",
    "print(lengths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a7fd2e",
   "metadata": {},
   "source": [
    "## Throughput Tips\n",
    "\n",
    "- Increase `num_workers` to parallelize CPU preprocessing (benchmark per machine).\n",
    "- Enable `pin_memory=True` when transferring batches to GPU.\n",
    "- Measure time per batch before assuming the model is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e847c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "start = time.perf_counter()\n",
    "for _ in range(5):\n",
    "    for xb, yb in loader:\n",
    "        _ = (xb, yb)\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Average step time: {elapsed / (5 * len(loader)):.6f}s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce6be45",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Vision Pipeline Blueprint\n",
    "\n",
    "Generate synthetic RGB images, apply augmentations and normalization, and build train/validation loaders. Explain how you would swap in a real dataset (e.g., CIFAR-10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41dd66fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class SyntheticImages(Dataset):\n",
    "    def __init__(self, num_images=200):\n",
    "        self.data = torch.rand(num_images, 3, 32, 32)\n",
    "        self.targets = torch.randint(0, 10, (num_images,))\n",
    "        # TODO: define train/eval transforms\n",
    "        self.training = True\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        self.training = mode\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # TODO: apply transforms based on mode\n",
    "        raise NotImplementedError\n",
    "\n",
    "# TODO: split dataset into train/validation loaders with augmentations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdbc371",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torchvision.transforms as T\n",
    "\n",
    "class SyntheticImages(Dataset):\n",
    "    def __init__(self, num_images=200):\n",
    "        self.data = torch.rand(num_images, 3, 32, 32)\n",
    "        self.targets = torch.randint(0, 10, (num_images,))\n",
    "        self.train_t = T.Compose([\n",
    "            T.RandomHorizontalFlip(),\n",
    "            T.RandomCrop(32, padding=4),\n",
    "            T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "        self.eval_t = T.Compose([\n",
    "            T.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "        ])\n",
    "        self.training = True\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        self.training = mode\n",
    "        return self\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        transforms = self.train_t if self.training else self.eval_t\n",
    "        return transforms(self.data[idx]), self.targets[idx]\n",
    "\n",
    "dataset = SyntheticImages(200)\n",
    "train_ds, val_ds = random_split(dataset, [160, 40], generator=torch.Generator().manual_seed(21))\n",
    "\n",
    "dataset.train(True)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "dataset.train(False)\n",
    "val_loader = DataLoader(val_ds, batch_size=32)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4bb1575",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- PyTorch Data Loading: https://pytorch.org/docs/stable/data.html\n",
    "- TorchData datapipes for streaming scenarios\n",
    "- NVIDIA DALI for GPU-accelerated preprocessing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
