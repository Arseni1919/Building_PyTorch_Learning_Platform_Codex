{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions Deep Dive\n",
    "\n",
    "Loss functions turn objectives into gradients. Understanding their behaviour is essential before we tackle attention mechanisms, transformers, and Mixture-of-Experts models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Match task types (regression, classification, probabilistic) to appropriate losses.\n",
    "- Interpret loss landscapes to anticipate optimization behaviour.\n",
    "- Implement masking and custom losses where built-ins fall short.\n",
    "- Build intuition for the losses used in attention and transformer architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Landscape Overview\n",
    "\n",
    "| Task | Common Loss | Notes |\n",
    "|------|-------------|-------|\n",
    "| Regression | `MSELoss`, `L1Loss`, `SmoothL1Loss` | Choose L1 for robustness to outliers, MSE for smooth gradients. |\n",
    "| Binary classification | `BCEWithLogitsLoss` | Combines sigmoid + BCE with numerical stability. |\n",
    "| Multi-class classification | `CrossEntropyLoss` | Expects raw logits and class indices. |\n",
    "| Multi-label | `BCEWithLogitsLoss` | Independent sigmoid per label. |\n",
    "| Probabilistic | `KLDivLoss`, `NLLLoss` | Operate on log probabilities and distributions. |\n",
    "| Sequence modeling | `CrossEntropyLoss` + masking | Ignore padding tokens to avoid skewed gradients. |\n",
    "\n",
    "Whenever you introduce a new objective, fit it into this table first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "preds_reg = torch.tensor([2.5, 0.0, -1.5])\n",
    "targets_reg = torch.tensor([3.0, -0.5, -1.0])\n",
    "mse = F.mse_loss(preds_reg, targets_reg)\n",
    "mae = F.l1_loss(preds_reg, targets_reg)\n",
    "print(f\"MSE: {mse:.4f}\")\n",
    "print(f\"MAE: {mae:.4f}\")\n",
    "\n",
    "diffs = np.linspace(-3, 3, 200)\n",
    "plt.plot(diffs, diffs ** 2, label=\"MSE contribution\")\n",
    "plt.plot(diffs, np.abs(diffs), label=\"MAE contribution\")\n",
    "plt.legend(); plt.xlabel(\"Error\"); plt.ylabel(\"Loss contribution\"); plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Essentials\n",
    "\n",
    "`CrossEntropyLoss` expects raw logits and integer class labels. Do not apply softmax manually; the loss combines log-softmax and negative log-likelihood in a single stable operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[2.0, 0.5, -1.0], [0.1, 2.3, 1.0]])\n",
    "targets = torch.tensor([0, 2])\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "print(f\"Cross entropy: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking Padding Tokens\n",
    "\n",
    "Sequence-to-sequence models contain padded positions. Use `ignore_index` (default `-100`) so padding does not skew gradient updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[2.0, 0.1, -1.0], [1.2, 1.5, 0.3]])\n",
    "targets = torch.tensor([0, -100])\n",
    "masked_loss = F.cross_entropy(logits, targets, ignore_index=-100)\n",
    "print(f\"Masked loss: {masked_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Binary Cross-Entropy Stability\n",
    "\n",
    "Compute the binary cross-entropy loss manually (sigmoid + `binary_cross_entropy`) and compare it with `BCEWithLogitsLoss` to verify numerical stability.\n",
    "\n",
    "Attempt the starter cell before revealing the hidden solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.tensor([[1.2], [-0.7], [0.4]])\n",
    "targets = torch.tensor([[1.0], [0.0], [1.0]])\n",
    "\n",
    "# TODO: compute manual BCE (sigmoid + binary_cross_entropy) and stable BCE with logits\n",
    "# print both values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "logits = torch.tensor([[1.2], [-0.7], [0.4]])\n",
    "targets = torch.tensor([[1.0], [0.0], [1.0]])\n",
    "\n",
    "manual = F.binary_cross_entropy(torch.sigmoid(logits), targets)\n",
    "stable = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "print(f\"Manual BCE: {manual.item():.6f}\")\n",
    "print(f\"Stable BCE: {stable.item():.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Loss Example (Focal Loss)\n",
    "\n",
    "Focal loss down-weights easy examples. It shows up in detection models and can help with class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def focal_loss(logits, targets, alpha=0.25, gamma=2.0):\n",
    "    bce = F.binary_cross_entropy_with_logits(logits, targets, reduction=\"none\")\n",
    "    probs = torch.sigmoid(logits)\n",
    "    pt = torch.where(targets == 1, probs, 1 - probs)\n",
    "    return (alpha * (1 - pt) ** gamma * bce).mean()\n",
    "\n",
    "print(focal_loss(torch.tensor([[1.2], [-0.7], [0.4]]), torch.tensor([[1.0], [0.0], [1.0]])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Loss Selection Utility\n",
    "\n",
    "Implement `choose_loss(task_type, **details)` that returns both the initialized loss function and a short justification. Cover regression, multi-class, multi-label, language modeling with padding, and knowledge distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_loss(task_type: str, **details):\n",
    "    # TODO: map task types to loss functions and explanations\n",
    "    raise NotImplementedError\n",
    "\n",
    "examples = [\n",
    "    (\"regression\", {}),\n",
    "    (\"multiclass\", {\"num_classes\": 5}),\n",
    "    (\"multilabel\", {\"num_labels\": 3}),\n",
    "    (\"language_modeling\", {\"ignore_index\": -100}),\n",
    "    (\"distillation\", {}),\n",
    "]\n",
    "\n",
    "for task, kwargs in examples:\n",
    "    loss_fn, reason = choose_loss(task, **kwargs)\n",
    "    print(task, type(loss_fn), reason)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def choose_loss(task_type: str, **details):\n",
    "    task_type = task_type.lower()\n",
    "    if task_type == \"regression\":\n",
    "        return torch.nn.MSELoss(), \"Squared error supplies smooth gradients for regression.\"\n",
    "    if task_type == \"multiclass\":\n",
    "        if \"num_classes\" not in details:\n",
    "            raise ValueError(\"Specify num_classes for multiclass classification.\")\n",
    "        return torch.nn.CrossEntropyLoss(), \"CrossEntropy combines log-softmax with NLL.\"\n",
    "    if task_type == \"multilabel\":\n",
    "        return torch.nn.BCEWithLogitsLoss(), \"Independent sigmoid heads support multi-label targets.\"\n",
    "    if task_type == \"language_modeling\":\n",
    "        ignore_index = details.get(\"ignore_index\", -100)\n",
    "        return torch.nn.CrossEntropyLoss(ignore_index=ignore_index), \"Ignore padding tokens when computing loss.\"\n",
    "    if task_type == \"distillation\":\n",
    "        return (\n",
    "            lambda student, teacher: F.kl_div(\n",
    "                F.log_softmax(student, dim=-1),\n",
    "                F.softmax(teacher, dim=-1),\n",
    "                reduction=\"batchmean\",\n",
    "            ),\n",
    "            \"KL divergence aligns student distributions with teacher outputs.\",\n",
    "        )\n",
    "    raise ValueError(f\"Unknown task type: {task_type}\")\n",
    "\n",
    "examples = [\n",
    "    (\"regression\", {}),\n",
    "    (\"multiclass\", {\"num_classes\": 5}),\n",
    "    (\"multilabel\", {\"num_labels\": 3}),\n",
    "    (\"language_modeling\", {\"ignore_index\": -100}),\n",
    "    (\"distillation\", {}),\n",
    "]\n",
    "\n",
    "for task, kwargs in examples:\n",
    "    loss_fn, reason = choose_loss(task, **kwargs)\n",
    "    print(task, type(loss_fn), reason)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- PyTorch Loss Functions: https://pytorch.org/docs/stable/nn.html#loss-functions\n",
    "- Lin et al. (2017) – Focal Loss for Dense Object Detection\n",
    "- Label smoothing strategies in transformer-based language models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
