{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Workflows & Experiment Structure\n",
    "\n",
    "You now have data and models. This notebook orchestrates them into robust training and evaluation loops so experiments are reproducible and debuggable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Implement clean training and validation loops with logging hooks.\n",
    "- Monitor gradients, losses, and learning rates.\n",
    "- Integrate gradient clipping, schedulers, and early stopping.\n",
    "- Build patterns that extend to large-scale and attention-heavy systems later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anatomy of a Training Step\n",
    "\n",
    "1. Fetch a batch from the dataloader.\n",
    "2. Run the forward pass.\n",
    "3. Compute the loss.\n",
    "4. Backpropagate gradients.\n",
    "5. Update parameters and reset gradients.\n",
    "\n",
    "Instrumentation (logging gradients, losses, learning rate) wraps these steps without cluttering the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "x = torch.linspace(-3, 3, steps=256).unsqueeze(1)\n",
    "y = torch.sin(x) + 0.1 * torch.randn_like(x)\n",
    "train_ds = TensorDataset(x, y)\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "\n",
    "model = nn.Sequential(nn.Linear(1, 32), nn.ReLU(), nn.Linear(32, 1))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "    return total_loss / len(loader.dataset)\n",
    "\n",
    "epoch_loss = train_one_epoch(model, train_loader, optimizer, criterion)\n",
    "print(f\"Epoch loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Monitoring\n",
    "\n",
    "Exploding or vanishing gradients quietly sabotage experiments. Logging gradient norms helps you react early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_norm(model: nn.Module):\n",
    "    total = 0.0\n",
    "    for p in model.parameters():\n",
    "        if p.grad is None:\n",
    "            continue\n",
    "        total += p.grad.data.norm(2).item() ** 2\n",
    "    return total ** 0.5\n",
    "\n",
    "xb, yb = next(iter(train_loader))\n",
    "loss = criterion(model(xb), yb)\n",
    "loss.backward()\n",
    "print(f\"Gradient norm: {gradient_norm(model):.4f}\")\n",
    "optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Validation Loop\n",
    "\n",
    "Implement an evaluation helper that disables gradients, computes the loss and mean absolute error, and returns both metrics.\n",
    "\n",
    "Attempt the starter cell before expanding the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    # TODO: accumulate loss and MAE without gradients\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            total_loss += loss.item() * xb.size(0)\n",
    "            total_mae += torch.mean(torch.abs(preds - yb)).item() * xb.size(0)\n",
    "    model.train()\n",
    "    n = len(loader.dataset)\n",
    "    return total_loss / n, total_mae / n\n",
    "\n",
    "val_loss, val_mae = evaluate(model, train_loader, criterion)\n",
    "print(f\"Val loss: {val_loss:.4f}, Val MAE: {val_mae:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checkpointing & Scheduling\n",
    "\n",
    "- Save weights **and** optimizer state so you can resume without losing learning rate schedules.\n",
    "- Pair AdamW with cosine or polynomial schedulers for large models.\n",
    "- Log learning rates to detect unexpected schedule jumps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {\n",
    "    \"model\": model.state_dict(),\n",
    "    \"optimizer\": optimizer.state_dict(),\n",
    "    \"epoch\": 1,\n",
    "    \"notes\": \"sine regression warm-up\",\n",
    "}\n",
    "torch.save(checkpoint, \"notebooks/beginner/checkpoint_sine.pt\")\n",
    "print(\"Checkpoint saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Experiment Harness\n",
    "\n",
    "Create an `Experiment` class that manages training, validation, gradient clipping, optional learning-rate scheduler, and simple logging. Demonstrate usage on the sine regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, criterion, grad_clip=None, scheduler=None):\n",
    "        # TODO: store components and initialize history\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train(self, epochs):\n",
    "        # TODO: implement training with validation metrics and optional early stopping\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    def __init__(self, model, optimizer, train_loader, val_loader, criterion, grad_clip=None, scheduler=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.grad_clip = grad_clip\n",
    "        self.scheduler = scheduler\n",
    "        self.history = {\"train\": [], \"val\": []}\n",
    "\n",
    "    def _train_one_epoch(self):\n",
    "        self.model.train()\n",
    "        total = 0.0\n",
    "        for xb, yb in self.train_loader:\n",
    "            preds = self.model(xb)\n",
    "            loss = self.criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            if self.grad_clip is not None:\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            total += loss.item() * xb.size(0)\n",
    "        return total / len(self.train_loader.dataset)\n",
    "\n",
    "    def _evaluate(self):\n",
    "        self.model.eval()\n",
    "        total = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in self.val_loader:\n",
    "                total += self.criterion(self.model(xb), yb).item() * xb.size(0)\n",
    "        self.model.train()\n",
    "        return total / len(self.val_loader.dataset)\n",
    "\n",
    "    def train(self, epochs):\n",
    "        best_val = float(\"inf\")\n",
    "        patience = 3\n",
    "        patience_counter = 0\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self._train_one_epoch()\n",
    "            val_loss = self._evaluate()\n",
    "            self.history[\"train\"].append(train_loss)\n",
    "            self.history[\"val\"].append(val_loss)\n",
    "            if self.scheduler is not None:\n",
    "                self.scheduler.step()\n",
    "            print(f\"Epoch {epoch+1}: train={train_loss:.4f}, val={val_loss:.4f}\")\n",
    "            if val_loss < best_val - 1e-4:\n",
    "                best_val = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping triggered\")\n",
    "                    break\n",
    "        return self.history\n",
    "\n",
    "experiment = Experiment(model, optimizer, train_loader, train_loader, criterion, grad_clip=1.0)\n",
    "experiment.train(epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- PyTorch Training Loop Recipes: https://pytorch.org/tutorials/recipes/recipes.html\n",
    "- TorchMetrics, TensorBoard, Weights & Biases for richer experiment tracking\n",
    "- “A Recipe for Training Neural Networks” by Andrej Karpathy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
