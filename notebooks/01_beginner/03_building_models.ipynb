{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Models with `nn.Module`\n",
    "\n",
    "Data pipelines are ready—now we architect models. This notebook takes a top-down view: start with design principles, then dive into reusable components that plug into the broader workflow.\n",
    "\n",
    "_Environment note:_ Recommendations mirror best practices through October 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Implement custom `nn.Module` classes and combine them with `nn.Sequential`.\n",
    "- Apply normalization, activation, and dropout layers thoughtfully.\n",
    "- Introduce residual connections to stabilize deeper networks.\n",
    "- Prepare more advanced modules (for CNNs, transformers, and MoE models) without rewriting boilerplate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modular Design Principles\n",
    "\n",
    "- **Encoders** convert raw inputs into speaker representations.\n",
    "- **Heads** map representations to task-specific outputs.\n",
    "- **Utilities** such as normalization and residual pathways keep activations healthy.\n",
    "\n",
    "Organizing models into these roles makes debugging easier and encourages reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = SimpleMLP(4, 32, 1)\n",
    "dummy = torch.randn(8, 4)\n",
    "print(model(dummy).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization & Stability\n",
    "\n",
    "- Use Kaiming initialization for ReLU-heavy stacks, Xavier for near-linear transitions.\n",
    "- Normalization layers help manage activation scales.\n",
    "- Residual connections (`x + f(x)`) make optimization of deep networks tractable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_init(module: nn.Module):\n",
    "    if isinstance(module, nn.Linear):\n",
    "        nn.init.kaiming_uniform_(module.weight, nonlinearity=\"relu\")\n",
    "        nn.init.zeros_(module.bias)\n",
    "\n",
    "model.apply(kaiming_init);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualMLPBlock(nn.Module):\n",
    "    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.ff(self.norm(x))\n",
    "\n",
    "block = ResidualMLPBlock(32, 64)\n",
    "print(block(torch.randn(4, 32)).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Linear-Norm-Activation Block\n",
    "\n",
    "Create a module that applies Linear → BatchNorm1d → GELU and returns both the activated output and the pre-activation values. This pattern reappears in attention feed-forward layers.\n",
    "\n",
    "Work through the starter cell before expanding the hidden solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNormActivation(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        # TODO: define linear, batchnorm, activation layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: return tuple (post_activation, pre_activation)\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class LinearNormActivation(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.norm = nn.BatchNorm1d(out_dim)\n",
    "        self.act = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        pre = self.linear(x)\n",
    "        normed = self.norm(pre)\n",
    "        out = self.act(normed)\n",
    "        return out, pre\n",
    "\n",
    "block = LinearNormActivation(4, 8)\n",
    "out, pre = block(torch.randn(3, 4))\n",
    "print(out.shape, pre.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Blocks to Networks\n",
    "\n",
    "- Stack residual MLP blocks to prototype transformer feed-forward sublayers.\n",
    "- Combine CNN backbones with task-specific heads using the same module patterns.\n",
    "- Keep modules decoupled so you can swap them during experimentation without rewriting glue code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Configurable Feedforward Network\n",
    "\n",
    "Create an MLP that accepts a list of hidden dimensions, optional dropout, and residual connections when consecutive layer widths match. Provide a `forward_with_intermediates` method for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurableMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        # TODO: construct layers and residual bookkeeping\n",
    "\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward_with_intermediates(self, x):\n",
    "        # TODO: return (output, activations)\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class ConfigurableMLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        dims = [input_dim] + list(hidden_dims) + [output_dim]\n",
    "        layers = []\n",
    "        self.residual_flags = []\n",
    "        for idx in range(len(dims) - 1):\n",
    "            in_dim, out_dim = dims[idx], dims[idx + 1]\n",
    "            layers.append(nn.Linear(in_dim, out_dim))\n",
    "            if idx < len(dims) - 2:\n",
    "                layers.append(nn.LayerNorm(out_dim))\n",
                    "                layers.append(nn.GELU())\n",
    "                if dropout > 0:\n",
    "                    layers.append(nn.Dropout(dropout))\n",
    "                self.residual_flags.append(in_dim == out_dim)\n",
    "        self.residual_flags.append(False)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.forward_with_intermediates(x)\n",
    "        return out\n",
    "\n",
    "    def forward_with_intermediates(self, x):\n",
    "        activations = []\n",
    "        residual = x\n",
    "        idx = 0\n",
    "        flag_idx = 0\n",
    "        while idx < len(self.layers):\n",
    "            layer = self.layers[idx]\n",
    "            x = layer(x)\n",
    "            idx += 1\n",
    "            if idx < len(self.layers) and isinstance(self.layers[idx], nn.LayerNorm):\n",
    "                norm = self.layers[idx]\n",
    "                act = self.layers[idx + 1]\n",
    "                x = act(norm(x))\n",
    "                idx += 2\n",
    "                if idx < len(self.layers) and isinstance(self.layers[idx], nn.Dropout):\n",
    "                    x = self.layers[idx](x)\n",
    "                    idx += 1\n",
    "                if self.residual_flags[flag_idx]:\n",
    "                    x = x + residual\n",
    "                residual = x\n",
    "                flag_idx += 1\n",
    "            activations.append(x)\n",
    "        return x, activations\n",
    "\n",
    "mlp = ConfigurableMLP(10, [32, 32, 16], 1, dropout=0.1)\n",
    "out, acts = mlp.forward_with_intermediates(torch.randn(4, 10))\n",
    "print(out.shape, len(acts))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- PyTorch `nn` Module Reference: https://pytorch.org/docs/stable/nn.html\n",
    "- He et al. (2015) – Deep Residual Learning for Image Recognition\n",
    "- FastAI lessons on reusable model blocks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
