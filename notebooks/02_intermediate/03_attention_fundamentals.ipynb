{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Fundamentals: Self, Cross, and Multi-Head\n",
    "\n",
    "Attention lets models focus on the most relevant context without the recurrence bottleneck. This notebook develops scaled dot-product attention step-by-step and prepares you to assemble full transformer blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Derive scaled dot-product attention and understand each tensor transformation.\n",
    "- Visualize attention weights to interpret where models focus.\n",
    "- Implement self-attention, cross-attention, and multi-head variants.\n",
    "- Build modular attention blocks with residual connections for use in transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention Pipeline\n",
    "\n",
    "1. Project inputs into queries (Q), keys (K), and values (V).\n",
    "2. Compute compatibility scores `QK^T / sqrt(d_k)`.\n",
    "3. Apply softmax to obtain attention weights.\n",
    "4. Weight values to produce context vectors.\n",
    "\n",
    "Scaling by `sqrt(d_k)` keeps gradients stable when dimensionality grows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(5)\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = q @ k.transpose(-2, -1) / d_k ** 0.5\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output = weights @ v\n",
    "    return output, weights\n",
    "\n",
    "q = torch.randn(2, 4, 8)\n",
    "k = torch.randn(2, 4, 8)\n",
    "v = torch.randn(2, 4, 8)\n",
    "\n",
    "context, weights = scaled_dot_product_attention(q, k, v)\n",
    "print(context.shape, weights.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Attention Weights\n",
    "\n",
    "Heatmaps help you see which tokens influence each other. This is invaluable when debugging or explaining model decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4, 3))\n",
    "attn = weights[0].detach()\n",
    "im = ax.imshow(attn, cmap=\"viridis\")\n",
    "for i in range(attn.size(0)):\n",
    "    for j in range(attn.size(1)):\n",
    "        ax.text(j, i, f\"{attn[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"white\")\n",
    "ax.set_xlabel(\"Key index\")\n",
    "ax.set_ylabel(\"Query index\")\n",
    "fig.colorbar(im, ax=ax)\n",
    "plt.title(\"Self-attention weights (sample 0)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "Multiple heads allow the model to attend to different representation subspaces. Each head performs attention independently before recombining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.q_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = torch.nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        context = x if context is None else context\n",
    "        bsz = x.size(0)\n",
    "\n",
    "        def reshape(tensor):\n",
    "            return tensor.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q = reshape(self.q_proj(x))\n",
    "        k = reshape(self.k_proj(context))\n",
    "        v = reshape(self.v_proj(context))\n",
    "\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, -1, self.embed_dim)\n",
    "        return self.out_proj(attn_output), attn_weights\n",
    "\n",
    "mha = MultiHeadAttention(embed_dim=32, num_heads=4)\n",
    "out, w = mha(torch.randn(2, 5, 32))\n",
    "print(out.shape, w.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Causal Mask\n",
    "\n",
    "Autoregressive models (language generation) require a causal mask so each position attends only to previous positions. Create a lower-triangular mask and apply it to the scaled dot-product attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 4\n",
    "# TODO: create causal mask of shape (1, 1, seq_len, seq_len) and apply attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "seq_len = 4\n",
    "mask = torch.tril(torch.ones(1, 1, seq_len, seq_len, dtype=torch.bool))\n",
    "masked_context, masked_weights = scaled_dot_product_attention(\n",
    "    q.view(2, 1, 4, 8),\n",
    "    k.view(2, 1, 4, 8),\n",
    "    v.view(2, 1, 4, 8),\n",
    "    mask=mask,\n",
    ")\n",
    "print(masked_context.shape, masked_weights.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Attention\n",
    "\n",
    "Cross-attention connects decoder queries to encoder outputs. This is the bridge between the sequence modeling notebook and the transformer architecture coming next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_outputs = torch.randn(2, 7, 32)\n",
    "decoder_states = torch.randn(2, 5, 32)\n",
    "cross_out, cross_weights = mha(decoder_states, context=encoder_outputs)\n",
    "print(cross_out.shape, cross_weights.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Attention Block\n",
    "\n",
    "Implement an `AttentionBlock` that includes layer normalization, residual connections, optional cross-attention, and a position-wise feed-forward network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, cross_attention=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # TODO: compose self-attention, optional cross-attention, feed-forward, and residual paths\n",
    "\n",
    "    def forward(self, x, context=None, mask=None, context_mask=None):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class AttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, cross_attention=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads) if cross_attention else None\n",
    "        self.norm1 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = torch.nn.LayerNorm(embed_dim)\n",
    "        self.ff = torch.nn.Sequential(\n",
    "            torch.nn.Linear(embed_dim, ff_dim),\n",
    "            torch.nn.GELU(),\n",
    "            torch.nn.Dropout(dropout),\n",
    "            torch.nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None, context_mask=None):\n",
    "        residual = x\n",
    "        attn_out, _ = self.self_attn(self.norm1(x), mask=mask)\n",
    "        x = residual + self.dropout(attn_out)\n",
    "\n",
    "        if self.cross_attn is not None and context is not None:\n",
    "            residual = x\n",
    "            cross_out, _ = self.cross_attn(self.norm2(x), context=context, mask=context_mask)\n",
    "            x = residual + self.dropout(cross_out)\n",
    "\n",
    "        residual = x\n",
    "        ff_out = self.ff(self.norm3(x))\n",
    "        x = residual + self.dropout(ff_out)\n",
    "        return x\n",
    "\n",
    "block = AttentionBlock(embed_dim=32, num_heads=4, ff_dim=64, cross_attention=True)\n",
    "dummy_x = torch.randn(2, 6, 32)\n",
    "dummy_context = torch.randn(2, 7, 32)\n",
    "print(block(dummy_x, context=dummy_context).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- Vaswani et al. (2017) “Attention Is All You Need”\n",
    "- PyTorch `nn.MultiheadAttention` documentation\n",
    "- Annotated Transformer blog posts for step-by-step derivations\n",
    "- “A Primer in BERTology” for attention interpretability techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
