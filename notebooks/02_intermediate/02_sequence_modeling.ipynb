{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Modeling with RNNs, LSTMs, and GRUs\n",
    "\n",
    "Sequences—text, audio, time series—require models that respect order and temporal dependencies. This notebook connects embeddings and recurrent networks to the attention mechanisms you will implement next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Construct embeddings and recurrent blocks (RNN, LSTM, GRU) in PyTorch.\n",
    "- Handle variable-length sequences with packing utilities.\n",
    "- Implement teacher forcing in a simple sequence-to-sequence model.\n",
    "- Build a bi-LSTM tagger with masking and accuracy metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representing Tokens with Embeddings\n",
    "\n",
    "Embeddings turn discrete tokens into continuous vectors that the network can differentiate. Initialize them randomly; gradients will shape them during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "torch.manual_seed(2)\n",
    "\n",
    "vocab_size = 20\n",
    "embedding_dim = 16\n",
    "embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "tokens = torch.randint(0, vocab_size, (4, 6))\n",
    "embedded = embedding(tokens)\n",
    "print(embedded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vanilla RNN vs. GRU vs. LSTM\n",
    "\n",
    "Recurrent networks process one timestep at a time, maintaining hidden state. GRUs/LSTMs mitigate the vanishing gradient problem with gates.\n",
    "\n",
    "Monitor hidden-state norms to diagnose stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 6\n",
    "hidden_size = 32\n",
    "\n",
    "rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "gru = nn.GRU(embedding_dim, hidden_size, batch_first=True)\n",
    "lstm = nn.LSTM(embedding_dim, hidden_size, batch_first=True)\n",
    "\n",
    "rnn_out, _ = rnn(embedded)\n",
    "gru_out, _ = gru(embedded)\n",
    "lstm_out, _ = lstm(embedded)\n",
    "\n",
    "for name, out in {\"RNN\": rnn_out, \"GRU\": gru_out, \"LSTM\": lstm_out}.items():\n",
    "    norms = out.norm(dim=-1).mean().item()\n",
    "    print(f\"{name} hidden-state mean norm: {norms:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Packed Sequences\n",
    "\n",
    "Variable-length batches waste compute if you process padding. Use `pack_padded_sequence`/`pad_packed_sequence` to skip padded timesteps.\n",
    "\n",
    "Implement the packing/unpacking workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "lengths = torch.tensor([6, 4, 2, 1])\n",
    "sorted_idx = torch.argsort(lengths, descending=True)\n",
    "sorted_embedded = embedded[sorted_idx]\n",
    "\n",
    "# TODO: pack, run through GRU, then unpack back to padded form\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "lengths = torch.tensor([6, 4, 2, 1])\n",
    "sorted_idx = torch.argsort(lengths, descending=True)\n",
    "sorted_embedded = embedded[sorted_idx]\n",
    "packed = pack_padded_sequence(sorted_embedded, lengths[sorted_idx], batch_first=True)\n",
    "packed_out, hidden = gru(packed)\n",
    "unpadded, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "print(unpadded.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence-to-Sequence Skeleton\n",
    "\n",
    "Before attention, encoder-decoder RNNs were the backbone of translation. The example below uses teacher forcing (feeding ground-truth tokens during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.encoder = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.GRU(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_emb = self.embedding(src)\n",
    "        tgt_emb = self.embedding(tgt)\n",
    "        _, hidden = self.encoder(src_emb)\n",
    "        outputs, _ = self.decoder(tgt_emb, hidden)\n",
    "        return self.classifier(outputs)\n",
    "\n",
    "model = Seq2Seq(vocab_size=30, embed_dim=18, hidden_dim=32)\n",
    "src = torch.randint(0, 30, (2, 5))\n",
    "tgt = torch.randint(0, 30, (2, 6))\n",
    "print(model(src, tgt).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Bi-LSTM Tagger\n",
    "\n",
    "Implement a bi-directional LSTM tagger with masking for padding positions and an accuracy metric that ignores padded tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_tags):\n",
    "        super().__init__()\n",
    "        # TODO: initialize embedding, bi-LSTM, dropout, classifier\n",
    "\n",
    "    def forward(self, tokens, lengths):\n",
    "        # TODO: run packed sequence through bi-LSTM, return logits\n",
    "        raise NotImplementedError\n",
    "\n",
    "def token_accuracy(logits, targets, mask):\n",
    "    # TODO: compute accuracy ignoring masked positions\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class BiLSTMTagger(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_tags, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_tags)\n",
    "\n",
    "    def forward(self, tokens, lengths):\n",
    "        embedded = self.embedding(tokens)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.lstm(packed)\n",
    "        outputs, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        outputs = self.dropout(outputs)\n",
    "        return self.classifier(outputs)\n",
    "\n",
    "def token_accuracy(logits, targets, mask):\n",
    "    preds = logits.argmax(dim=-1)\n",
    "    correct = (preds == targets) & mask\n",
    "    total = mask.sum()\n",
    "    return correct.sum().float() / total.float()\n",
    "\n",
    "torch.manual_seed(4)\n",
    "tokens = torch.randint(1, 50, (3, 7))\n",
    "lengths = torch.tensor([7, 5, 6])\n",
    "tags = torch.randint(0, 8, (3, 7))\n",
    "mask = torch.arange(tokens.size(1)).expand_as(tokens) < lengths.unsqueeze(1)\n",
    "\n",
    "tagger = BiLSTMTagger(50, 32, 64, 8)\n",
    "logits = tagger(tokens, lengths)\n",
    "print(\"Accuracy:\", token_accuracy(logits, tags, mask).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- PyTorch NLP tutorials: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\n",
    "- Bahdanau et al. (2015) – Neural Machine Translation by Jointly Learning to Align and Translate\n",
    "- Consider torchtext or Hugging Face Datasets for large-scale sequence inputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
