{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture: Encoder-Decoder & Positional Encoding\n",
    "\n",
    "Attention removes recurrence bottlenecks, but the transformer architecture adds structure—positional encodings, stacked encoder and decoder blocks, and masking strategies. This notebook assembles those pieces so you can build both encoder-decoder and decoder-only models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Implement sinusoidal and learned positional encodings.\n",
    "- Build transformer encoder and decoder layers using attention blocks.\n",
    "- Apply padding and causal masks correctly.\n",
    "- Construct a small decoder-only model capable of greedy generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Attention Blocks\n",
    "\n",
    "The transformer relies on multi-head attention and residual feed-forward networks. We package the implementation here so the notebook is self-contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask=None):\n",
    "    d_k = q.size(-1)\n",
    "    scores = q @ k.transpose(-2, -1) / d_k ** 0.5\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    output = weights @ v\n",
    "    return output, weights\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None):\n",
    "        context = x if context is None else context\n",
    "        bsz = x.size(0)\n",
    "\n",
    "        def reshape(tensor):\n",
    "            return tensor.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        q = reshape(self.q_proj(x))\n",
    "        k = reshape(self.k_proj(context))\n",
    "        v = reshape(self.v_proj(context))\n",
    "\n",
    "        attn_output, attn_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(bsz, -1, self.embed_dim)\n",
    "        return self.out_proj(attn_output), attn_weights\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, cross_attention=False, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(embed_dim, num_heads)\n",
    "        self.cross_attn = MultiHeadAttention(embed_dim, num_heads) if cross_attention else None\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, context=None, mask=None, context_mask=None):\n",
    "        residual = x\n",
    "        attn_out, _ = self.self_attn(self.norm1(x), mask=mask)\n",
    "        x = residual + self.dropout(attn_out)\n",
    "\n",
    "        if self.cross_attn is not None and context is not None:\n",
    "            residual = x\n",
    "            cross_out, _ = self.cross_attn(self.norm2(x), context=context, mask=context_mask)\n",
    "            x = residual + self.dropout(cross_out)\n",
    "\n",
    "        residual = x\n",
    "        ff_out = self.ff(self.norm3(x))\n",
    "        x = residual + self.dropout(ff_out)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional Encodings\n",
    "\n",
    "Without positional encodings, a transformer cannot distinguish between permutations of the same tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * (-math.log(10000.0) / embed_dim))\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer(\"pe\", pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, : x.size(1)]\n",
    "\n",
    "encoding = SinusoidalPositionalEncoding(32)\n",
    "tokens = torch.zeros(1, 50, 32)\n",
    "encoded = encoding(tokens)[0].detach()\n",
    "plt.plot(encoded[:, :4])\n",
    "plt.title(\"First four sinusoidal dimensions\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Positional Embeddings\n",
    "\n",
    "Learned positional embeddings often help when sequence lengths stay within a known range, such as in language modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedPositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=512):\n",
    "        super().__init__()\n",
    "        self.positional_embedding = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        nn.init.normal_(self.positional_embedding, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        length = x.size(1)\n",
    "        return x + self.positional_embedding[:, :length]\n",
    "\n",
    "learned_encoding = LearnedPositionalEncoding(32)\n",
    "print(learned_encoding(torch.zeros(1, 10, 32)).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder and Decoder Layers\n",
    "\n",
    "Encoder layers include self-attention and feed-forward sublayers. Decoder layers add masked self-attention plus cross-attention over encoder outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block = AttentionBlock(embed_dim, num_heads, ff_dim, cross_attention=False, dropout=dropout)\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        return self.block(src, mask=src_mask)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.block = AttentionBlock(embed_dim, num_heads, ff_dim, cross_attention=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        return self.block(tgt, context=memory, mask=tgt_mask, context_mask=memory_mask)\n",
    "\n",
    "encoder_layer = EncoderLayer(32, 4, 64)\n",
    "decoder_layer = DecoderLayer(32, 4, 64)\n",
    "memory = encoder_layer(torch.randn(2, 7, 32))\n",
    "out = decoder_layer(torch.randn(2, 6, 32), memory)\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Stack\n",
    "\n",
    "We now combine positional encodings, encoder/decoder layers, and masking into a full encoder-decoder transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=32, num_heads=4, ff_dim=64, num_layers=2, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.positional_encoding = SinusoidalPositionalEncoding(embed_dim)\n",
    "        self.encoders = nn.ModuleList([\n",
    "            EncoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.output = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def make_causal_mask(self, size, device):\n",
    "        return torch.tril(torch.ones(1, 1, size, size, device=device, dtype=torch.bool))\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None):\n",
    "        device = src.device\n",
    "        src_emb = self.positional_encoding(self.embedding(src))\n",
    "        memory = src_emb\n",
    "        for layer in self.encoders:\n",
    "            memory = layer(memory, src_mask)\n",
    "\n",
    "        tgt_emb = self.positional_encoding(self.embedding(tgt))\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = self.make_causal_mask(tgt.size(1), device)\n",
    "        output = tgt_emb\n",
    "        for layer in self.decoders:\n",
    "            output = layer(output, memory, tgt_mask, src_mask)\n",
    "\n",
    "        return self.output(self.norm(output))\n",
    "\n",
    "model = Transformer(vocab_size=100)\n",
    "src = torch.randint(0, 100, (2, 7))\n",
    "tgt = torch.randint(0, 100, (2, 6))\n",
    "print(model(src, tgt).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Label Smoothing\n",
    "\n",
    "Label smoothing improves generalization by preventing the model from becoming overconfident. Implement a label-smoothed cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_smoothed_nll_loss(logits, targets, smoothing=0.1):\n",
    "    # TODO: implement log-softmax, smooth target distribution, and compute cross-entropy\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def label_smoothed_nll_loss(logits, targets, smoothing=0.1):\n",
    "    num_classes = logits.size(-1)\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    with torch.no_grad():\n",
    "        true_dist = torch.zeros_like(log_probs)\n",
    "        true_dist.fill_(smoothing / (num_classes - 1))\n",
    "        true_dist.scatter_(2, targets.unsqueeze(-1), 1.0 - smoothing)\n",
    "    loss = (-true_dist * log_probs).sum(dim=-1).mean()\n",
    "    return loss\n",
    "\n",
    "logits = torch.randn(2, 6, 50)\n",
    "targets = torch.randint(0, 50, (2, 6))\n",
    "print(label_smoothed_nll_loss(logits, targets))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Mini Decoder-Only Transformer\n",
    "\n",
    "Implement a decoder-only model (GPT-style) with token embeddings, learned positional encodings, stacked decoder layers, weight tying between embeddings and output projection, and greedy generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, num_heads=4, ff_dim=128, num_layers=3, max_len=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # TODO: define embeddings, positional encodings, decoder layers, tie weights\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        # TODO: apply causal mask and return logits\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def generate(self, prompt, max_new_tokens=20):\n",
    "        # TODO: greedy generation loop\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class MiniGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=64, num_heads=4, ff_dim=128, num_layers=3, max_len=64, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "        self.token_embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embed = nn.Parameter(torch.zeros(1, max_len, embed_dim))\n",
    "        nn.init.normal_(self.position_embed, mean=0.0, std=0.02)\n",
    "        self.decoders = nn.ModuleList([\n",
    "            DecoderLayer(embed_dim, num_heads, ff_dim, dropout) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.output = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        self.output.weight = self.token_embed.weight  # weight tying\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tokens):\n",
    "        bsz, seq_len = tokens.size()\n",
    "        device = tokens.device\n",
    "        positions = self.position_embed[:, :seq_len]\n",
    "        x = self.token_embed(tokens) + positions\n",
    "        x = self.dropout(x)\n",
    "        mask = torch.tril(torch.ones(1, 1, seq_len, seq_len, device=device, dtype=torch.bool))\n",
    "        for layer in self.decoders:\n",
    "            x = layer(x, x, tgt_mask=mask, memory_mask=None)\n",
    "        x = self.norm(x)\n",
    "        return self.output(x)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, prompt, max_new_tokens=20):\n",
    "        self.eval()\n",
    "        generated = prompt.clone()\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits = self.forward(generated)\n",
    "            next_token = logits[:, -1, :].argmax(dim=-1, keepdim=True)\n",
    "            generated = torch.cat([generated, next_token], dim=1)\n",
    "        return generated\n",
    "\n",
    "model = MiniGPT(vocab_size=200)\n",
    "sample = torch.randint(0, 200, (2, 12))\n",
    "print(model(sample).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- Vaswani et al. (2017) “Attention Is All You Need”\n",
    "- Annotated Transformer (Harvard NLP)\n",
    "- GPT-2 Technical Report for decoder-only training heuristics\n",
    "- PyTorch `nn.Transformer` tutorial for reference implementations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
