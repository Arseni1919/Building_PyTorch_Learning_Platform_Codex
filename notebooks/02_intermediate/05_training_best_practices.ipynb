{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Best Practices & Performance Toolkit\n",
    "\n",
    "Optimizers and architectures only go so far—training strategy determines whether models converge efficiently. This notebook collects practical techniques for scheduling, mixed precision, gradient hygiene, and instrumentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Visualize learning rate schedules and understand when to use them.\n",
    "- Apply mixed precision with gradient scaling.\n",
    "- Implement gradient clipping, accumulation, and simple monitoring utilities.\n",
    "- Build a training harness that combines these ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Schedules\n",
    "\n",
    "Schedulers smooth optimization by ramping up (warmup) and decaying learning rates. Plot the schedule to confirm it behaves as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = nn.Sequential(nn.Linear(16, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)\n",
    "\n",
    "lrs = []\n",
    "for step in range(20):\n",
    "    lrs.append(optimizer.param_groups[0][\"lr\"])\n",
    "    scheduler.step()\n",
    "\n",
    "plt.plot(lrs, marker=\"o\")\n",
    "plt.title(\"CosineAnnealingWarmRestarts schedule\")\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Precision Training\n",
    "\n",
    "Mixed precision boosts throughput by using lower precision where safe. Autocast + GradScaler handle casting and scaling automatically. When CUDA is unavailable, the code falls back to full precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "def train_step(x, y):\n",
    "    optimizer.zero_grad()\n",
    "    with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "        preds = model(x)\n",
    "        loss = torch.nn.functional.mse_loss(preds, y)\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    return loss.item()\n",
    "\n",
    "x = torch.randn(32, 16)\n",
    "y = torch.randn(32, 1)\n",
    "print(f\"Loss: {train_step(x, y):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Gradient Monitoring Utility\n",
    "\n",
    "Create a helper that logs per-layer gradient norms and identifies the largest contributor. This aids debugging when gradients explode or vanish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_stats(model: nn.Module):\n",
    "    # TODO: return dict of {name: norm} and print largest contributor\n",
    "    raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "def gradient_stats(model: nn.Module):\n",
    "    norms = {}\n",
    "    max_name, max_norm = None, 0.0\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is None:\n",
    "            continue\n",
    "        norm = param.grad.data.norm(2).item()\n",
    "        norms[name] = norm\n",
    "        if norm > max_norm:\n",
    "            max_name, max_norm = name, norm\n",
    "    if max_name:\n",
    "        print(f\"Largest gradient: {max_name} -> {max_norm:.4f}\")\n",
    "    return norms\n",
    "\n",
    "loss = train_step(x, y)\n",
    "gradient_stats(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Accumulation & Clipping\n",
    "\n",
    "Accumulation simulates large batches on small hardware. Clipping prevents exploding gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_accumulation(loader, accumulation_steps=2, grad_clip=1.0):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    for step, (xb, yb) in enumerate(loader):\n",
    "        with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "            preds = model(xb)\n",
    "            loss = torch.nn.functional.mse_loss(preds, yb) / accumulation_steps\n",
    "        scaler.scale(loss).backward()\n",
    "        if (step + 1) % accumulation_steps == 0:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "dummy_loader = [(torch.randn(16, 16), torch.randn(16, 1)) for _ in range(4)]\n",
    "train_with_accumulation(dummy_loader)\n",
    "print(\"Accumulation step completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini Task – Early Stopper\n",
    "\n",
    "Implement an `EarlyStopper` that tracks validation improvements, supports a patience parameter, and exposes `should_stop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0.0):\n",
    "        # TODO: initialize fields\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, metric):\n",
    "        # TODO: return True when training should stop\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=3, min_delta=0.0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best = float(\"inf\")\n",
    "        self.counter = 0\n",
    "\n",
    "    def update(self, metric):\n",
    "        if metric < self.best - self.min_delta:\n",
    "            self.best = metric\n",
    "            self.counter = 0\n",
    "            return False\n",
    "        self.counter += 1\n",
    "        return self.counter >= self.patience\n",
    "\n",
    "stopper = EarlyStopper(patience=2, min_delta=0.01)\n",
    "for metric in [0.5, 0.45, 0.44, 0.44, 0.43]:\n",
    "    if stopper.update(metric):\n",
    "        print(\"Early stop triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comprehensive Exercise – Training Harness\n",
    "\n",
    "Create a `Trainer` class that supports mixed precision, gradient accumulation, schedulers, checkpointing, and simple callbacks. Demonstrate usage on the dummy regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler=None, grad_clip=None, accumulation_steps=1, checkpoint_path=None):\n",
    "        # TODO: store components and initialize GradScaler\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def train_epoch(self, loader):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit(self, train_loader, val_loader=None, epochs=1):\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": [
     "solution",
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, optimizer, scheduler=None, grad_clip=None, accumulation_steps=1, checkpoint_path=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.grad_clip = grad_clip\n",
    "        self.accumulation_steps = accumulation_steps\n",
    "        self.checkpoint_path = checkpoint_path\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())\n",
    "        self.history = {\"train\": [], \"val\": []}\n",
    "\n",
    "    def train_epoch(self, loader):\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        self.optimizer.zero_grad()\n",
    "        for step, (xb, yb) in enumerate(loader):\n",
    "            with torch.cuda.amp.autocast(enabled=torch.cuda.is_available()):\n",
    "                preds = self.model(xb)\n",
    "                loss = torch.nn.functional.mse_loss(preds, yb) / self.accumulation_steps\n",
    "            self.scaler.scale(loss).backward()\n",
    "            if (step + 1) % self.accumulation_steps == 0:\n",
    "                if self.grad_clip is not None:\n",
    "                    self.scaler.unscale_(self.optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.grad_clip)\n",
    "                self.scaler.step(self.optimizer)\n",
    "                self.scaler.update()\n",
    "                self.optimizer.zero_grad()\n",
    "                if self.scheduler is not None:\n",
    "                    self.scheduler.step()\n",
    "            total_loss += loss.item() * xb.size(0) * self.accumulation_steps\n",
    "        return total_loss / len(loader.dataset)\n",
    "\n",
    "    def evaluate(self, loader):\n",
    "        self.model.eval()\n",
    "        total = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in loader:\n",
    "                preds = self.model(xb)\n",
    "                total += torch.nn.functional.mse_loss(preds, yb).item() * xb.size(0)\n",
    "        self.model.train()\n",
    "        return total / len(loader.dataset)\n",
    "\n",
    "    def fit(self, train_loader, val_loader=None, epochs=1):\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = self.train_epoch(train_loader)\n",
    "            val_loss = None\n",
    "            if val_loader is not None:\n",
    "                val_loss = self.evaluate(val_loader)\n",
    "            self.history[\"train\"].append(train_loss)\n",
    "            self.history[\"val\"].append(val_loss)\n",
    "            print(f\"Epoch {epoch+1}: train={train_loss:.4f} val={val_loss}\")\n",
    "            if self.checkpoint_path:\n",
    "                torch.save({\n",
    "                    \"model\": self.model.state_dict(),\n",
    "                    \"optimizer\": self.optimizer.state_dict(),\n",
    "                    \"scaler\": self.scaler.state_dict(),\n",
    "                    \"history\": self.history,\n",
    "                    \"epoch\": epoch + 1,\n",
    "                }, self.checkpoint_path)\n",
    "        return self.history\n",
    "\n",
    "trainer = Trainer(model, optimizer, scheduler, grad_clip=1.0, accumulation_steps=2)\n",
    "history = trainer.fit(dummy_loader, dummy_loader, epochs=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "- PyTorch Performance Tuning Guide: https://pytorch.org/tutorials/recipes/recipes.html#optimizing-your-model\n",
    "- NVIDIA Mixed Precision Training documentation\n",
    "- “Don’t Decay the Learning Rate, Increase the Batch Size” (Smith et al.)\n",
    "- PyTorch Profiler for deeper instrumentation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
